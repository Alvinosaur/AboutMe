<!DOCTYPE html>
<html lang="en">

<!-- Imports -->

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Personal Website">
  <meta name="author" content="Alvin Shek">

  <title>Real-Time Collaborative Robot Handling</title>

  <!-- Bootstrap Core CSS -->
  <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom Fonts -->
  <link href="../../vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,300italic,400italic,700italic"
    rel="stylesheet" type="text/css">
  <link href="../../vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../css/project/wowchemy.min.css">
  <link rel="manifest" href="../../index.webmanifest">
  <link href="../../css/stylish-portfolio.css" rel="stylesheet">
  <link rel="icon" href="../../main_icon.svg">
  <link rel="apple-touch-icon" href="../../main_icon.svg">
  <link rel="canonical" href="https://alvinosaur.github.io/AboutMe/projects/idl_project">
  <link rel="alternate" hreflang="en-us" href="https://alvinosaur.github.io/AboutMe/projects/idl_project">

  <link rel="stylesheet" href="../../css/project/custom_project.css">

  <!-- Javascript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

  <!------------------------------------------------------------------------>

  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="description"
    content="In top-5 among 46 projects in the course @ CMU 11-785 - Introduction to Deep Learning (Fall 2020) - [Project Gallery](https://deeplearning.cs.cmu.edu/F20/gallery/gallery.html)">

  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

  <meta name="theme-color" content="hsl(267Â°, 95%, 76%)">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"
    integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css"
    crossorigin="anonymous" title="hl-light" disabled>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css"
    crossorigin="anonymous" title="hl-dark">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css"
    integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA=="
    crossorigin="anonymous">

  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">

</head>



<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#projects">Projects</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger"
            href="https://alvinosaur.github.io/AboutMe/photography/main.html">Photography</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#contact_me">Contact Me</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="page-body bg-light">
    <article class="article article-project">

      <div class="article-container pt-3">
        <h1>Real-Time Collaborative Robot Handling</h1>
        <h3>Rui Chen*, Alvin Shek*, Changliu Liu</h3>
        <h4>Submitted to IEEE Transactions on Robotics (T-RO) 2022</h4>

        <div class="article-metadata">

          <span class="article-date">

            Dec 11, 2021
          </span>

        </div>

        <div class="btn-links mb-3">

          <a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/abs/2112.06020" target="_blank"
            rel="noopener">
            Paper
          </a>

          <a class="btn btn-outline-primary my-1 mr-1" href="https://docs.google.com/presentation/d/1dpVPVZyZDihGukUKCJ_WTgYKMGkMlYMI-ddcYEZjMfU/edit?usp=sharing" target="_blank" rel="noopener">
            Slides
          </a>

        </div>

      </div>
      <div class="article-header article-container featured-image-wrapper mt-4 mb-4"
        style="max-width: 720px; max-height: 369px;">
        <div style="position: relative">
          <img src="main.png" alt="" class="featured-image">
          <span class="article-header-caption"></span>
        </div>
      </div>
      <br>

      <div class="article-container">

        <div class="article-style">
          <h3>Intro</h3>

          <p>With the advancement of robotic technologies, robots are
            getting out of cages and directly working with humans. One
            immediate application of collaborative robot (cobot) is material handling where the robot moves or presents a workpiece
            under human commands in real time. It is particularly useful
            when the material to handle is heavy, as in automotive assembly, or when the material needs to stay untouched by human,
            as in the case of food production, or when there are safety
            risks, as in the case of dangerous liquid.</p>

          <p>Existing interfaces with robots such as keyboards and joysticks require either tedious integration efforts
            or programming expertise and can hardly be used in real-
            time. Other interfaces such as voice or static hand gestures are 
          more user intuitive and require no technical skills to be learned beforehand. Static gestures, however, are limited to generating
        discrete motion primitives rather than real-time, continuous movements and thus limit the flexibility of robot behavior. Our approach is unique in this aspect by handling continuous, dynamic gestures.</p>
        </div>

        <h3>Framework</h3>

        <div class="article-style">
          <!-- <div class="article-header article-container featured-image-wrapper mt-4 mb-4"
            style="max-width: 720px; max-height: 400px;">
            
          </div> -->
          <div style="position: relative">
            <video controls>
              <source src="demo.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <!-- <p>Link: https://drive.google.com/file/d/1P4Wjw1ygfPAbNCCGdUU9bskClGYozgML/view?usp=sharing</p> -->
            <!-- <span class="article-header-caption"></span> -->
          </div>
          <p>
            The above video shows me demonstrating the final performance 
            of our model. As I perform various hand motions that involve 
            multiple Cartesian dimesnsions at the same time (ex: translating from left to right while rotating), we see that 
            the model is able to interpret this correctly and infer the correct robot motion. Specfically, our model takes as input 6D
            translational and rotational velocities of each finger and palm of the right hand, which we calculate from OpenPose finger keypoint positions. Our model then outputs 6D translational and rotational 
            velocities of the robot end-effector. 
          </p>
          <div style="position: relative">
            <img src="data_collection.png" alt="Data Collection">
            <!-- <span class="article-header-caption"></span> -->
          </div>
          <p>
            To train this model, we collected data from a series of human-human collaboration trials. One user would watch an animation of
            the desired motion of a box and would try generating the hand motion
            to match this. A second user would interpret the first user's hand motions and move a real, physical box whose 6D pose would be tracked. As a result, we would obtain both the hand motion input to the model as
            well as the ground truth object motion representing the robot end effector.
          </p>
        </div>

        <h3>Probabilistic Formulation: Generative Process</h3>
        <p>
          There are several key challenges in interpreting human commands. First, people think and act differently, and this can cause
          different users' hand motion styles to vary drastically. Machine Learning models expect inputs to come from a distribution that matches what was seen at training. The diversity in user styles means
          that a new user's style may not have been seen at training, which can cause unpredictable model outputs. To handle this diversity, 
          we draw inspiration from <a href="https://arxiv.org/abs/1901.05761 " target="_blank" rel="noopener">Attentive Neural Processes</a> by training our model to condition on some prior "context" or calibration data collected by previous users. The idea is that the model can reference previously seen data to help it interpret new user data. This is done through an attention mechanism over LSTM hidden states that capture latent features of a time series of both hand and desired object motion. Please see the "Encoder Cell" under Section V.A of our paper for more details.
        </p>
        <p>Another challenge is in modeling human motion uncertainty. Given the exact same desired object motion, a human may execute different hand motions in different trials. Our model needs to handle this stochasticity in the expert demonstration. We formulate our problem as a generative process with the goal of maximizing the expected log-probability of
        the ground truth object motion when conditioned on the input hand motion. Please see Eqn. 4 in our paper for details.
        <p>
          A final challenge is that our robot will operate in close proximity to humans. From a human perspective, it is 
          easy to predict the behavior of smooth motions but not 
          sharp, sudden motions. As a result, we need our learned robot policy
          to also output smooth, continuous motions. We achieve this by formulating our generative process to condition also on the previous 
          model output. This way, our model's future output is dependent on its previous output. We train this dependency using Teacher Forcing. 
        </p>

        <h3>Conclusion</h3>
        <p>
          This project helped me realize the many challenges of interacting with humans that collaborative robots must address. Many more issues
          exist that were not covered in this project, and I am currently exploring these further as a part of my masters thesis.
        </p>


        <div class="media author-card content-widget-hr">


          <a href="https://alvinosaur.github.io/AboutMe"><img class="avatar mr-3 avatar-circle"
              src="../../profile_pic.jpg" alt="Alvin Shek"></a>


          <div class="media-body">
            <h5 class="card-title"><a href="https://alvinosaur.github.io/AboutMe">Alvin Shek</a></h5>
            <h6 class="card-subtitle">Robotics Masters Student @ CMU</h6>
            <p class="card-text">Robotics, Computer Vision, Deep Learning, Reinforcement Learning.</p>
            <ul class="network-icon" aria-hidden="true">

              <li>
                <a href="mailto:ashek@andrew.cmu.edu">
                  <i class="fas fa-envelope"></i>
                </a>
              </li>

              <li>
                <a href="https://github.com/Alvinosaur" target="_blank" rel="noopener">
                  <i class="fab fa-github"></i>
                </a>
              </li>

              <li>
                <a href="https://www.linkedin.com/in/alvinshek/" target="_blank" rel="noopener">
                  <i class="fab fa-linkedin"></i>
                </a>
              </li>

              <li>
                <a href="../../Alvin_Shek_Resume.pdf" target="_blank" rel="noopener">
                  <i class="far fa-address-book"></i>
                </a>
              </li>

            </ul>

          </div>
        </div>


        <div class="project-related-pages content-widget-hr">

        </div>
      </div>
    </article>
  </div>


</body>

</html>