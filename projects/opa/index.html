<!DOCTYPE html>
<html lang="en">

<!-- Imports -->

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Personal Website">
  <meta name="author" content="Alvin Shek">

  <title>Learning from Physical Human Feedback:An Object-Oriented One-Shot Adaptation Method</title>

  <!-- Bootstrap Core CSS -->
  <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom Fonts -->
  <link href="../../vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,300italic,400italic,700italic"
    rel="stylesheet" type="text/css">
  <link href="../../vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../css/project/wowchemy.min.css">
  <link rel="manifest" href="../../index.webmanifest">
  <link href="../../css/stylish-portfolio.css" rel="stylesheet">
  <link rel="icon" href="../../main_icon.svg">
  <link rel="apple-touch-icon" href="../../main_icon.svg">
  <link rel="canonical" href="https://alvinosaur.github.io/AboutMe/projects/idl_project">
  <link rel="alternate" hreflang="en-us" href="https://alvinosaur.github.io/AboutMe/projects/idl_project">

  <link rel="stylesheet" href="../../css/project/custom_project.css">

  <!-- Javascript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

  <!------------------------------------------------------------------------>

  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="description"
    content="Project page for the paper: Learning from Physical Human Feedback: <br>An Object-Centric One-Shot Adaptation Method">

  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

  <meta name="theme-color" content="hsl(267°, 95%, 76%)">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"
    integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css"
    crossorigin="anonymous" title="hl-light" disabled>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css"
    crossorigin="anonymous" title="hl-dark">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css"
    integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA=="
    crossorigin="anonymous">

  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">

</head>



<body id="page-top">

  <!-- Navigation -->
  <!-- <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#projects">Projects</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger"
            href="https://alvinosaur.github.io/AboutMe/photography/main.html">Photography</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#contact_me">Contact Me</a>
        </li>
      </ul>
    </div>
  </nav> -->



  <div class="page-body bg-white">
    <article class="article article-project">

      <div class="article-container pt-3">
        <center>
          <h3>Learning from Physical Human Feedback: <br>An Object-Centric One-Shot Adaptation Method</h3>
        </center>
        <!-- 
          <div class="table-like authors" style="justify-content:space-evenly;max-width:800px;margin:auto;">
            <div><center><span><a href="https://alvinosaur.github.io/AboutMe/" target="_blank"
              rel="noopener">Alvin Shek</a></span></center>
              <center><span><a href="https://ruichen.pub/" target="_blank"
                rel="noopener">Rui Chen</a></span></center>
                <center><span><a href="http://www.cs.cmu.edu/~cliu6/" target="_blank"
                  rel="noopener">Changliu Liu</a></span></center>
             </div>
    
          </div> -->

        <center>
          <!-- <br> -->
          <nobr><a href="https://alvinosaur.github.io/AboutMe/" target="_blank" rel="noopener">Alvin Shek</a></nobr>   
          <nobr><a href="https://ruichen.pub/" target="_blank" rel="noopener">Rui Chen</a></nobr>    <nobr><a
              href="http://www.cs.cmu.edu/~cliu6/" target="_blank" rel="noopener">Changliu Liu</a></nobr>
          <br>
          <nobr>Carnegie Mellon University</nobr>
          <!-- <img style="vertical-align:middle" src="mcp_teaser.png" width="100%" height="inherit">		 -->
        </center>

        <!-- <h4></h4> -->

        <div class="article-metadata">

          <span class="article-date">

            Mar 13, 2022
          </span>

        </div>

        <div class="btn-links mb-3">

          <a class="btn btn-outline-primary my-1 mr-1" href="https://youtu.be/rBi9v9m3wJ8" target="_blank"
            rel="noopener">
            Video
          </a>

          <a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/abs/2203.04951" target="_blank"
            rel="noopener">
            Paper
          </a>

          <a class="btn btn-outline-primary my-1 mr-1" href="" target="_blank" rel="noopener">
            Code (released by March 27 (sorry!))
          </a>

        </div>

      </div>
      <div class="article-header article-container featured-image-wrapper mt-4 mb-4"
        style="max-width: 720px; max-height: 369px;">
        <div style="position: relative">
          <div class="videoWrapper">
            <!-- Copy & Pasted from YouTube -->
            <!-- Source: https://css-tricks.com/fluid-width-video/ -->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/rBi9v9m3wJ8"
              title="YouTube video player" frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen></iframe>
          </div>
          <span class="article-header-caption"></span>
        </div>
      </div>
      <br>

      <div class="article-container">
        <img src="thumbnail.png" alt="" class="featured-image">
        <div class="article-style">
          <font size="4">
            <p>
              For robots to be effectively deployed in novel environments and tasks, they must be able to understand the
              feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate
              additional preferences. Existing methods either require repeated episodes of interactions or assume prior
              known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these
              assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical
              interventions in <em>relation to specific objects</em>. Our method, Object Preference Adaptation (OPA), is
              composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2)
              online-updating only certain weights in the model according to human feedback. The key to our fast, yet
              simple adaptation is that general interaction dynamics between agents and objects are fixed, and only
              object-specific preferences are updated. Our adaptation occurs online, requires only one human
              intervention (one-shot), and produces new behaviors never seen during training. Trained on cheap synthetic
              data instead of expensive human demonstrations, our policy demonstrates impressive adaptation to human
              perturbations on challenging, realistic tasks in our user study. <a
                href="https://arxiv.org/abs/2203.04951" target="_blank" rel="noopener">
                More details provided in the paper.
              </a>
            </p>
          </font>
        </div>

        <h3>Ablation Studies</h3>

        <h4>Zero-shot Transfer to Different Environment Settings</h4>
        <div class="article-style">
          <font size="4">
            <p>
              A major advantage of defining our policy <em>relative</em> to objects and learning features for <em>object-centric behavior</em> is that our policy naturally handles different environment settings. This is crucial: robots should be flexible in their behavior when objects get shifted and rotated around. The video below shows an extension of task 3 from our paper that examines behavior of the policy as the scanner's pose changes. Given human correction in an initial setting A, the policy should still behave correctly in different settings B and C without the need for additional supervision (zero-shot).
              <div style="position: relative">
                <video controls>
                  <source src="handling_new_obj_2.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <figcaption>Four episodes are shown: (1) human correction in setting A, (2) updated robot policy in setting A, (3) same policy transferred to setting B, and (4) same policy transferred to setting C.</figcaption>
              </div>
              We can see that the robot's behavior can change dramatically 
              to handle changes to the scanner's pose. This shows the power 
              of learning behavior for specific objects. While this object-centric paradigm may not solve all tasks, we argue this is crucial for successful generalization.
            </p>
          </font>
        </div>

        <h4>"Singularities" of Potential Fields</h4>
        <div class="article-style">
          <font size="4">
            <p>
              In Section 3-B of the paper, we mentioned how the position relation network overall outputs a push-pull
              force
              on the agent. We allow
              this direction to be freely determined by the network. Potential field methods also use this approach, but
              constrain the direction to be parallel to each agent-object vector. Only magnitude and sign of the vector
              can change. This may seem like an intuitive way to enforce structure in the network and reduce complexity,
              but this constraint fails during “singularities” where no orthogonal component is available to avoid an
              obstacle lying in the same direction as the goal.

            <div style="position: relative">
              <video controls>
                <source src="fixed_vs_free.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <figcaption>Forced vs free force contributions</figcaption>
            </div>
            The above video compares "forced" and "free" direction behavior respectively. As the blue agent moves to the
            yellow goal in both cases, pay attention to the "force" contribution of the goal and the repel object shown
            as yellow and red arrows respectively.
            Notice how in the "forced" version, the model correctly predicts a force vector pointing away from the red
            repel
            object. However,
            since there is no orthogonal component, the blue agent cannot avoid moving straight through the repel object
            as the goal force vector dominates. On the right side, however, the force direction contributed by the
            repulsor object has an orthogonal component, allowing the agent to avoid the repulsor.

            </p>
          </font>
        </div>

        <h4>Scalability with Objects</h4>
        <div class="article-style">
          <font size="4">
            <p>
              Graph-based models and neural networks have an advantage of being invariant to the order and number of
              objects in a scene. We only trained our model's position relation network in scenarios with
              two other objects: repel and attract. Ideally, our model should be able to generalize to more objects.
              Here, we examine this generalization ability. The below video shows our model on scenarios with 4 objects:
              two repel, two attract. The text at the center
              of each object displays the attention value placed on that object in the format att: value.
            <div style="position: relative">
              <video controls>
                <source src="scale_4_objs.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <figcaption>Model behavior with <b>4 objects</b></figcaption>
            </div>
            Overall, our model seems
            to behave well. Also, looking at the first few seconds, we see that our model correctly places high
            attention on objects directly "ahead" of it and decreases attention once "past" them. This is because of our
            state-based feature Goal-Relative Distance (Section 3-C). Next, we increase the number of objects to 6:
            <div style="position: relative">
              <video controls>
                <source src="scale_6_objs.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <figcaption>Model behavior with <b>6 objects</b></figcaption>
            </div>
            Clearly, our model begins to suffer issues such as getting stuck in local optima when the force
            contributions of objects equalize. This shows that local optima's are still a problem when a method lacks
            the ability to plan in the future. Also, our model seems to lose its ability to prioritize objects that are
            nearby and gets repelled by objects very far away. We can see that both close and far away objects are given
            similar attention values. This is strange since one of the state-based features that we provide the model is
            size-relative distance (Section 3-C), so ideally this issue would not arise. We are investigating this, but
            possibly a work-around is to simply introduce a training curriculum during our model's pre-training phase.
            Initially only train with 2-object scenarios, and slowly increase this as training progresses.
            </p>
          </font>
        </div>

        <h4>Tuning the Adaptation Learning Rate</h4>
        <div class="article-style">
          <font size="4">
            <p>
              Coming Soon: Comparison of Stochastc Gradient Descent vs Learned Optimizer (Learn2Learn) vs Recursive
              Least Squares
            </p>
          </font>
        </div>


      </div>
      <br>
    </article>
  </div>


</body>

</html>