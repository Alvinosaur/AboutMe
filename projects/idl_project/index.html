<!DOCTYPE html>
<html lang="en">

<!-- Imports -->

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Personal Website">
  <meta name="author" content="Alvin Shek">

  <title>Exploring Meta-Learning for Robotics</title>

  <!-- Bootstrap Core CSS -->
  <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom Fonts -->
  <link href="../../vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,300italic,400italic,700italic"
    rel="stylesheet" type="text/css">
  <link href="../../vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../css/project/wowchemy.min.css">
  <link rel="manifest" href="../../index.webmanifest">
  <link href="../../css/stylish-portfolio.css" rel="stylesheet">
  <link rel="icon" href="../../main_icon.svg">
  <link rel="apple-touch-icon" href="../../main_icon.svg">
  <link rel="canonical" href="https://alvinosaur.github.io/AboutMe/projects/idl_project">
  <link rel="alternate" hreflang="en-us" href="https://alvinosaur.github.io/AboutMe/projects/idl_project">

  <link rel="stylesheet" href="../../css/project/custom_project.css">

  <!-- Javascript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

  <!------------------------------------------------------------------------>

  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="description"
    content="In top-5 among 46 projects in the course @ CMU 11-785 - Introduction to Deep Learning (Fall 2020) - [Project Gallery](https://deeplearning.cs.cmu.edu/F20/gallery/gallery.html)">

  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

  <meta name="theme-color" content="hsl(267°, 95%, 76%)">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"
    integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css"
    crossorigin="anonymous" title="hl-light" disabled>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css"
    crossorigin="anonymous" title="hl-dark">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css"
    integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA=="
    crossorigin="anonymous">

  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">

</head>



<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#projects">Projects</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger"
            href="https://alvinosaur.github.io/AboutMe/photography/main.html">Photography</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="https://alvinosaur.github.io/AboutMe/#contact_me">Contact Me</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="page-body bg-light">
    <article class="article article-project">

      <div class="article-container pt-3">
        <h1>Exploring Meta-Learning for Robotics</h1>

        <div class="article-metadata">

          <span class="article-date">

            Dec 8, 2020
          </span>

        </div>

        <div class="btn-links mb-3">

          <a class="btn btn-outline-primary my-1 mr-1" href="idl_final_report.pdf" target="_blank" rel="noopener">
            PDF
          </a>

          <a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/Team-Grasp/idl-project" target="_blank"
            rel="noopener">
            Code
          </a>

          <a class="btn btn-outline-primary my-1 mr-1" href="https://slides.com/rohanpandya-1/deck" target="_blank"
            rel="noopener">
            Slides
          </a>

          <a class="btn btn-outline-primary my-1 mr-1"
            href="https://www.youtube.com/watch?v=cp1g_qhH-Go&amp;feature=youtu.be" target="_blank" rel="noopener">
            Video
          </a>

        </div>

      </div>

      <div class="article-header article-container featured-image-wrapper mt-4 mb-4"
        style="max-width: 720px; max-height: 369px;">
        <div style="position: relative">
          <img src="trained_maml.gif" alt="" class="featured-image">
          <span class="article-header-caption">Task: Reach Red Target</span>
        </div>
      </div>
      <br>

      <div class="article-container">

        <div class="article-style">
          <p><em>In top-5 among 46 projects in the course @ CMU 11-785 - Introduction to Deep Learning (Fall
              2020)
              - <a href="https://deeplearning.cs.cmu.edu/F20/gallery/gallery.html" target="_blank"
                rel="noopener">Project
                Gallery</a></em></p>

          <h3>Intro</h3>

          <p>Reinforcement learning (RL) is a powerful learning technique that enables agents to learn useful
            policies by interacting directly with the environment. While this training approach doesn’t require
            labeled data, it is plagued with convergence issues and is highly
            sample inefficient. The learned policies are often
            very specific to the task at hand and are not generalizable to similar task spaces.</p>
          <p>Meta-Reinforcement Learning is one strategy that can mitigate these issues, enabling robots to
            acquire new
            skills much more quickly. Often described as “learning how to learn”, it allows agents to
            leverage
            prior
            experience much more effectively. Recently published papers in Meta Learning show impressive
            speed
            and
            sample efficiency improvements over traditional methods of relearning the task for slight
            variations
            in the
            task
            objectives. A concern with these Meta Learning methods was that their success was only achieved
            on
            relatively small modifications to the initial task.</p>
          <p>Another concern in RL, as highlighted by <a href="https://arxiv.org/abs/1709.06560" target="_blank"
              rel="noopener">Henderson et al.</a>, is reproducibility and lack of
            standardization of the
            metrics and
            approaches, which can lead to wide variations in reported vs observed performances. To alleviate
            that,
            benchmarking frameworks such as <a href="https://arxiv.org/abs/1910.10897" target="_blank"
              rel="noopener">Meta-World</a> and <a href="https://arxiv.org/abs/1909.12271" target="_blank"
              rel="noopener">RLBench</a> have been proposed that establish a common ground for a fair
            comparison between
            approaches. In this work, we aim to utilize the task variety proposed by <a
              href="https://arxiv.org/abs/1910.10897" target="_blank" rel="noopener">Meta-World</a>
            and
            compare PPO, a
            vanilla policy gradient algorithm, with their Meta Learning counterparts (MAML and Reptile). The
            objective
            is to verify the magnitude of success of meta-learning algorithms over the vanilla variants, and
            test them
            on a variety of complicated tasks to test their limits in responding to the size of task
            variations.
            We also
            introduce a new technique — Multi-headed Reptile, which proposes a novel approach to address
            some of
            the
            shortcomings of both these Meta-Learning techniques, with some computational implementation
            suggestions that
            prevent slowdown.
          </p>
        </div>

        <h3>Results</h3>

        <div class="article-header article-container featured-image-wrapper mt-4 mb-4"
          style="max-width: 720px; max-height: 369px;">
          <div style="position: relative">
            <img src="results_plots.png" alt="" class="featured-image">
            <!-- <span class="article-header-caption"></span> -->
          </div>
        </div>

        <div class="article-style">
          <p>
            The figure above compares the success rate of each model averaged
            over the 10 test tasks after initial training on other tasks. MAML and
            Reptile both converge to 80% success rate within 100 iterations, whereas Multi-headed Reptile and
            random-initialized (Vanilla PPO) require 150 and 225 iterations
            respectively.
          </p>
          <p>
            As theorized, both MAML and Reptile converge significantly more quickly than Vanilla PPO.
            Interestingly, even in the first iteration, these two meta-learning techniques achieve around 40%
            success rate, demonstrating their learned behaviour about the nature of the task distribution.
          </p>
          <p>
            However, we found that Multi-headed Reptile does not work as well as we expected, performing just
            as well as Vanilla PPO. Although Multi-headed Reptile converged in fewer iterations than Vanilla
            PPO, its poor initial performance indicates that it failed to initialize weights optimally. As a sanity
            check, Vanilla PPO indeed has poor initial performance due to randomly initialized weights.
          </p>

          <h3>Tools</h3>
          <ul>
            <li><a href="https://github.com/stepjam/RLBench" target="_blank" rel="noopener">RLBench</a> provided the
              baseline robot environment that we built upon.</li>
            <li><a href="https://github.com/DLR-RM/stable-baselines3" target="_blank"
                rel="noopener">Stablebaselines3</a> provided the vanilla PPO algorithm.
            </li>
            <li>
              <a href="https://docs.ray.io/en/master/ray-overview/index.html#parallelizing-python-java-functions-with-ray-tasks"
                target="_blank" rel="noopener">Ray</a>
              provided an easy solution to creating multiple, independent
              simulation workers to train different copies of the model on
              different tasks.
            </li>
            <li>
              <a href="https://wandb.ai/site" target="_blank" rel="noopener">Wandb</a>
              was critical to help us manage different training runs and keep
              track of experiments.
            </li>
          </ul>

          <h3>Contribution</h3>

          <p>
            I first built much of the initial training and evaluation pipeline,
            eventually getting PPO to train properly on the reach task. Due to
            the robotic application, I had to restrict the magnitude of the
            agent's output actions to be physically feasible.

            <a href="https://github.com/stepjam/RLBench/issues/52#issuecomment-754757967" target="_blank"
              rel="noopener">See this Git issue for more details.</a>
            Without this,
            many of the agents' actions would trigger errors in the simulator's
            inverse kinematics solver, and initial training would be extremely
            slow.
          </p>

          <p>
            I next wrote much of MAML and Reptile.
            <a href="https://github.com/Team-Grasp/idl-project/blob/reptile/maml.py" target="_blank" rel="noopener">See
              here for our implementation.</a>
            I also helped
            modify our architecture to utilize Ray for parallelization over
            independent tasks, as in the case of Multi-headed Reptile.
            <a href="https://github.com/Team-Grasp/idl-project/blob/f269fadcfa2275ca9cb9429dd5f6f293ca184960/run_maml.py#L92"
              target="_blank" rel="noopener">See here for hyperparameters.</a>
          </p>

        </div>

        <h3>Thoughts</h3>
        <p>
          Meta-learning certainly shows much promise, as indicated by the
          initial high success rate of MAML and Reptile. However, I'd like to
          see these algorithms' performance on more complicated tasks besides
          the basic reach task. These results unfortunately were achieved with a
          strong reward signal (Euclidean distance of End-effector (EE) from target),
          and since action space was delta EE position, the model in effect only
          had to learn the vector from current position to target. I wonder how
          use of a sparse reward would change performance. For robots to solve
          any general task in households or factories, rewards should not need
          to be hand-designed.

        </p>

        <div class="media author-card content-widget-hr">


          <a href="https://alvinosaur.github.io/AboutMe"><img class="avatar mr-3 avatar-circle"
              src="../../profile_pic.jpg" alt="Alvin Shek"></a>


          <div class="media-body">
            <h5 class="card-title"><a href="https://alvinosaur.github.io/AboutMe">Alvin Shek</a></h5>
            <h6 class="card-subtitle">Robotics Masters Student @ CMU</h6>
            <p class="card-text">Robotics, Computer Vision, Deep Learning, Reinforcement Learning.</p>
            <ul class="network-icon" aria-hidden="true">

              <li>
                <a href="mailto:ashek@andrew.cmu.edu">
                  <i class="fas fa-envelope"></i>
                </a>
              </li>

              <li>
                <a href="https://github.com/Alvinosaur" target="_blank" rel="noopener">
                  <i class="fab fa-github"></i>
                </a>
              </li>

              <li>
                <a href="https://www.linkedin.com/in/alvinshek/" target="_blank" rel="noopener">
                  <i class="fab fa-linkedin"></i>
                </a>
              </li>

              <li>
                <a href="../../Alvin_Shek_Resume.pdf" target="_blank" rel="noopener">
                  <i class="far fa-address-book"></i>
                </a>
              </li>

            </ul>

          </div>
        </div>


        <div class="project-related-pages content-widget-hr">

        </div>
      </div>
    </article>
  </div>


</body>

</html>